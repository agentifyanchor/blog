
[{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/","section":"Agentify Anchor","summary":"","title":"Agentify Anchor","type":"page"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/bot/","section":"Tags","summary":"","title":"Bot","type":"tags"},{"content":"In this blog post, I will guide you through how to create a bot-based message extension for Microsoft Teams using JavaScript, without the need for a Microsoft 365 Copilot license. While Microsoft 365 Copilot provides enhanced features for enterprise users, you can still build powerful bots using Ollama, a locally run model like Llama3.2, to handle natural language processing. By the end of this post, you’ll have a functional message extension that can perform tasks in Teams, powered by Ollama\u0026rsquo;s Llama3.2 model, all without the necessity of the Microsoft 365 Copilot license.\n1. Setting Up Ollama Locally Using Docker Compose # In this section, I’ll walk you through setting up Ollama locally using Docker Compose. This step-by-step guide will show you how to run Ollama with the Llama3.2 model, providing you with a local environment to process natural language queries for your Microsoft Teams bot.\nOur aporach is inspired from n8n self hosted ai strater kit. n8n-io/self-hosted-ai-starter-kit the main idea bhind is to separate main Ollama service from the initialization service that pulls models.\ngraph TD A[Ollama_Main_API_Service] --\u003e|Port 11434| B[Ollama_Model_Server_Llama_Models] B --\u003e C[Ollama_Pull_Models_Service] C --\u003e D[Downloads_Models_llama3.2_llama2_nomic_embed_text] A --\u003e|Exposes_API| B C --\u003e|Depends_on| A C --\u003e|Downloads_Models| B style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#ccf,stroke:#333,stroke-width:2px style C fill:#cfc,stroke:#333,stroke-width:2px style D fill:#fcf,stroke:#333,stroke-width:2px the main Ollama API service, runs the Ollama model server and exposes the API on port 11434.\nThe second instance (init-ollama) is responsible for pulling the models (e.g., llama3.2) when the container starts.\nThe step to run this is quwite simple juste download the docker compose file and run : docker compose \u0026ndash;profile cpu up note : if you rae interested to GPU you can upgrade this or inspiring from the n8n template So after a while we can check our service by runing curl command:\ncurl -X GET http://localhost:11434/v1/models if model(s) is availbale we should get a response like below: if model loaded sucefufly we can proced with runing some prompt:\ncurl -X POST http://localhost:11434/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello, Ollama!\u0026#34;}] }\u0026#39; 2. Updating the LlamaModel.ts Code # After setting up Ollama, I’ll walk you through the necessary updates to the LlamaModel.ts file. I’ve verride the completePrompt method to adjust the payload structure to properly interact with Ollama’s API endpoints. This update ensures that your bot is compatible with the message extension framework while utilizing Ollama for natural language processing.\nthe most importatnte defrence is the paylaod used to communicate with service endpoint so I change this from\n{ input_data: { input_string: result.output, parameters: template.config.completion } } to\nconst adjustedPayload = { model: \u0026#34;llama3.2:latest\u0026#34;, // Use the correct model identifier prompt: result.output.map(msg =\u0026gt; msg.content).join(\u0026#39; \u0026#39;), // Flatten messages into a single prompt string max_tokens: template.config.completion.max_tokens || 50, temperature: template.config.completion.temperature || 0.7, //completion_type: \u0026#34;chat\u0026#34;, }; so insted to make request like this\nawait this._httpClient.post\u0026lt;{ output: string }\u0026gt;(this.options.endpoint, { input_data: { input_string: result.output, parameters: template.config.completion } }); I use my adjustedPayload\nawait this.llamaModel[\u0026#39;_httpClient\u0026#39;].post(this.llamaModel.options.endpoint, adjustedPayload); 3. Integrating with Microsoft Teams Samples # To simplify my test I use the existing sample provided by micrososft teams ai team.\nTo strat using LlamaModelLocal with your ms teams engine agent just download file add it to your src project and add import new LlamaModelLocal insted of the LlamaModel like this : import { LlamaModelLocal } from \u0026ldquo;./oLlamaModel\u0026rdquo;; const model = new LlamaModelLocal insted of const model = new LlamaModel DEfine LLAMA_ENDPOINT in .env file to point on v1/completions LLAMA_ENDPOINT= http://localhost:11434/v1/completions\n4. Testing and Debugging with Microsoft Teams Developer Tools # Finally, we will be able to go through testing and debugging our bot using the Teams test Tool. We can strat debuging on just hit F5 make sure to run npm i or yarn i before strating debuging And here we go an instance of teams app test tool should open on http://localhost:54381/ browser should open in # So because I am in debug mode I can see the secret word so I am sheeting the bot This approach allows you to build Microsoft Teams bots using natural language processing locally with Ollama, without relying on Microsoft 365 Copilot licenses. It\u0026rsquo;s a great way to create powerful integrations with minimal setup and cost.\n","date":"20 January 2025","externalUrl":null,"permalink":"/posts/2025-01-20-building-a-free-bot-based-message-extension-for-microsoft-teams-without-microsoft-365-copilot-leveraging-ollama-with-llama32/","section":"Posts","summary":"","title":"Building a Free Custom Engine Agent for Microsoft Teams Without Microsoft 365 Copilot: Leveraging Ollama with Llama3.2","type":"posts"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/copilot/","section":"Tags","summary":"","title":"Copilot","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/llama3.2/","section":"Tags","summary":"","title":"Llama3.2","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/m365/","section":"Tags","summary":"","title":"M365","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/message-extension/","section":"Tags","summary":"","title":"Message Extension","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/microsoft-teams/","section":"Tags","summary":"","title":"Microsoft Teams","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/teams-toolkit/","section":"Tags","summary":"","title":"Teams Toolkit","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/devcontainer/","section":"Tags","summary":"","title":"DevContainer","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"To conclude this series on using UV for developing Python projects within a portable environment using containers, we will create a sample project and experiment with UV.\nIn our scenario, we will keep it simple. Our requirement is as follows: A store needs an API to return a list of products and filter these products by availability.\nTo get started, we need to set up our project and install the necessary dependencies. We will use FastAPI to create our API and Ruff as a linter to ensure code quality. Follow the steps below to initialize the project, install dependencies, and run the application.\nSteps to Follow: # Create the project. Install dependencies (FastAPI \u0026amp; uvicorn ). Install the linter (Ruff). Run the application. Explore the project structure and files. Create the Project # Run the command to initialize the project:\nuv init Store_api Explore the Project Structure # UV will create a project folder for us. It will generate the .gitignore file to manage which files or folders will be included or excluded from being pushed to our repository, .Python-version which mentions the global version of Python used, a hello world Python script, but the most important file here is pyproject.toml, which defines the project\u0026rsquo;s behavior and definition. pyproject.toml # [project] name = \u0026#34;store-api\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.12\u0026#34; dependencies = [] The pyproject.toml file specifies the required Python version for the project and lists the dependencies. Currently, the dependencies list is empty.\nAdd Dependencies (FastAPI \u0026amp; uvicorn) # As mentioned, we need FastAPI to create our API. To install the packages, navigate to the root project folder and run the following command:\nuv add fastapi uvicorn As you can see, it was so fast.\nTo quickly test the application, simply replace the content of hello.py with the following:\nfrom fastapi import FastAPI from typing import List, Optional from pydantic import BaseModel import uvicorn # Pydantic model for product response class ProductResponse(BaseModel): id: int name: str available: bool # Product class for internal data representation class Product: def __init__(self, id: int, name: str, available: bool): self.id = id self.name = name self.available = available # Sample products products = [ Product(1, \u0026#34;Jolly Jester Clown Wig\u0026#34;, True), Product(2, \u0026#34;Bozo the Clown Nose\u0026#34;, False), Product(3, \u0026#34;Circus Performer Clown Shoes\u0026#34;, True), Product(4, \u0026#34;Red Balloon Animal Kit\u0026#34;, True), Product(5, \u0026#34;Funny Clown Face Paint Set\u0026#34;, True), Product(6, \u0026#34;Mini Clown Horn\u0026#34;, False), Product(7, \u0026#34;Rainbow Clown Costume\u0026#34;, True), Product(8, \u0026#34;Clown Magician Hat\u0026#34;, True), Product(9, \u0026#34;Giggles the Clown Plush Doll\u0026#34;, True), Product(10, \u0026#34;Clown Comedy Seltzer Bottle\u0026#34;, False) ] # FastAPI app initialization app = FastAPI() @app.get(\u0026#34;/products\u0026#34;, response_model=List[ProductResponse]) def get_products(available: Optional[bool] = None): # If \u0026#39;available\u0026#39; query param is provided, filter products by availability filtered_products = products if available is None else [product for product in products if product.available == available] # Return products as list of Pydantic model instances return [ProductResponse(**product.__dict__) for product in filtered_products] @app.get(\u0026#34;/\u0026#34;) def main(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from store-api!\u0026#34;} if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) #main() # to run using command line `uv run uvicorn hello:app --port 8000` Installing and Using Tool (Ruff) # Every project needs some tooling like Ruff for linting. We can manage that by adding the tool as a dev dependency using uv add --dev ruff or just use uvx. Run the Application # To run the application, just use:\nuv run hello.py Click on the \u0026ldquo;Open in Browser\u0026rdquo; button from the popup. We don\u0026rsquo;t need to make any additional configuration for port forwarding. UV also creates and manages the .venv folder for us. upgrade / downgrade Python in a project # To upgrade or downgrade the Python version, simply update the version in the pyproject.toml file and run\nuv sync If you remember, we configured our initial environment to use Python 3.12, but we can use any version in our project. We don\u0026rsquo;t need to manage installation or set up multiple Python versions — UV does the job for us.\nIn this post, we have successfully set up a Python project using UV, ensuring that we can go live with our container and start our productive approach with Dev Containers.\nYou can find the complete source code for this demo on GitHub. If you’re a fan of Dockerfiles and prefer to manage your development environment manually, refer to this project, which uses the official UV Dockerfile definition.\nCheers! 🍻\n","date":"12 January 2025","externalUrl":null,"permalink":"/posts/2025-01-12-getting-started-with-uv-in-docker-step-3-explained/","section":"Posts","summary":"","title":"Getting Started with UV in Docker","type":"posts"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/podman/","section":"Tags","summary":"","title":"Podman","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/vscode/","section":"Tags","summary":"","title":"VSCode","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/wsl/","section":"Tags","summary":"","title":"WSL","type":"tags"},{"content":"","date":"8 January 2025","externalUrl":null,"permalink":"/posts/2025-01-08-run-mysql-in-a-docker-container-a-beginners-guide/","section":"Posts","summary":"","title":"Run MySQL in a Docker Container: A Beginner's Guide","type":"posts"},{"content":"In our series exploring how to set up a streamlined and organized way to start your Python development journey using the DevContainer VSCode extension and Docker (or WSL/Podman), we\u0026rsquo;ve already seen how simple it is to define our development environment using just the DevContainer extension. Now, we\u0026rsquo;re ready to move on to the next part of the series: Optimize Python Development with VSCode and Docker.\nRunning Our Environment for the First Time # In this post, we will run our development environment for the first time and start working with Python using an amazing package and project manager, uv.\nAfter generating the definition for our environment, we now have a devcontainer.json file structure. Let’s dig in to explore what we have in this setup:\nBreaking Down the devcontainer.json Structure # Image Section:\nThis section specifies the container image reference, which is hosted on the Microsoft Container Registry.\nNote: You can also use a local image if you prefer. use DockerFile or Docker compose file.\nFeatures Section:\nThink of this section as a way to enhance the environment with additional tools and packages. In our case, we’re using uv (with shellautocompletion option enbaled).\ncustomization Section: The customization section is used to fine-tune the container development environment. It allows us to install additional extensions that will only be loaded inside the container, apply specific settings related to VSCode preferences, and execute additional commands.\nFor more information on available features, visit the Dev Containers Features Documentation. # If you\u0026rsquo;re working with a rootless container, you can find relevant information. here.\nLaunching Our Environment for the First Time 🚀 # Now it\u0026rsquo;s time to launch our environment for the first time! To do this, simply click on the Dev Container icon and choose Reopen in Container.\nIn the background:\nAn image will be added to your Docker images list. A container should be running. A volume will be created. VSCode will reload, but this time, it will run under the container. Time to Verify # The Remote Indicator in the status bar will display additional information, such as the name of the container we\u0026rsquo;re currently in and the distribution of the image being used.\nVerifying Python Version and Testing uv # Let’s perform a couple more checks by verifying the Python version being used and testing if uv is running correctly.\nEverything seems good so far! The next step is to create our first project and run it. That’s exactly what we’ll cover in the next post.\nStay tuned!\n","date":"8 January 2025","externalUrl":null,"permalink":"/posts/2025-01-08-streamline-python-development-in-devcontainer-with-uv/","section":"Posts","summary":"","title":"Streamline Python Development in DevContainer with UV","type":"posts"},{"content":"In today\u0026rsquo;s world, Python is used for many purposes in large projects, especially in AI, LLM, and RAG. However, maintaining a healthy environment, managing compatibility, and handling packaging can be challenging tasks.\nFor this reason, using containers not only for deploying Python apps but also as a development environment could be a suitable approach.\nIn this post, we will show how simple it is to start developing Python projects without needing to install Python, manage multiple versions, create virtual environments, set environment variables, and handle other prerequisites.\nWe can achieve this directly from VSCode using just one extension.\nPresenting the Extension # We’ve probably all seen the .devcontainer folder and the devcontainer.json file in other projects. But what do they do?\nThe .devcontainer folder and its JSON file are used to define a containerized development environment. This setup ensures that all developers on a project have a consistent development environment, regardless of their local machine configurations. The .devcontainer configuration can specify the base image, extensions, and settings needed for the project.\nWe’re going to make this easy by generating the configuration files automatically—no need to craft them manually from scratch! 😉\nSo let\u0026rsquo;s get started\u0026hellip;\nPrerequisites # Before we dive in, make sure we have Docker and VSCode installed.\nStep 1: Installing the Extension # First, let’s install the necessary extension in VSCode. We can do this by visiting the Dev Container extension page or by searching for it directly in the extension manager in Visual Studio Code. Use the Ctrl+Shift+X shortcut to open the extension manager.\nStep 2: Generate the DevContainer Folder # After installing the extension, we should find an icon at the bottom left side of our IDE. Click on it and follow the wizard steps to generate the .devcontainer folder and configuration files.\nSelect Add Dev Container Configuration Files.\nThen select Add Configuration to Workspace. This will add the .devcontainer folder and related configuration to our current project directory.\nWe can choose from a variety of preset configurations. In our case, we will select Python 3.\nWe’ll be prompted to choose the Python version for your container. Pick any version you’re comfortable with (we’ll go with Python 3.12 bullseye).\nIn the following step, we can choose additional features. We’ll select uv, a great package and project manager that we’ll use further in this post.\nWe can also select additional configurations for the features, so let’s enable the shell autocompletion option.\nWe can skip the next step if we don\u0026rsquo;t want to use the Dependabot functionality. Dependabot helps us keep our dependencies up to date by automatically checking for updates and creating pull requests. After running the wizard, we’ll end up with the following root project structure.\n🎉 Once the .devcontainer folder is set up, there’s no need to build an image manually—the extension will handle that for us! We’re now ready to connect to the containerized environment and start using it as our development environment. We’ll explore this process in Part 2 of this post, so stay tuned!\nHappy coding, and see you in the next part!\n","date":"5 January 2025","externalUrl":null,"permalink":"/posts/2025-01-05-optimize-python-development-with-docker/","section":"Posts","summary":"","title":"Optimize Python Development with VSCode and Docker","type":"posts"},{"content":"","date":"5 January 2025","externalUrl":null,"permalink":"/tags/aboutme/","section":"Tags","summary":"","title":"Aboutme","type":"tags"},{"content":"","date":"5 January 2025","externalUrl":null,"permalink":"/tags/agentifyanchor/","section":"Tags","summary":"","title":"Agentifyanchor","type":"tags"},{"content":"Hello! 👋\nI\u0026rsquo;m a Microsoft Specialist 💻 with a deep passion for AI 🤖. Over the years, I\u0026rsquo;ve become fascinated by optimizing development environments, especially in the world of Python 🐍, Docker 🐋, and VSCode 🖥️. I’m always looking for ways to simplify complex tasks and make development smoother, which is why I love using containers and powerful tools to create consistent and portable setups.\nMy journey is all about exploring new technologies, automating workflows, and applying AI in innovative ways. I’m excited to share my insights and experiences with you, and I hope to learn and grow together in this tech journey! 🌟\nDon\u0026rsquo;t forget to follow me on Twitter. 🐦 ","date":"5 January 2025","externalUrl":null,"permalink":"/posts/discover-journey/","section":"Posts","summary":"","title":"Discover My Journey: About Me","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]