
[{"content":" Complete Generative AI Learning Resource for Beginners # You are inetrested by Generative AI and you dont know from where you can start or looking to refine your your expertise, we‚Äôve got you covered.\nI am excited to share with you a great comprehensive learning resource provided by Microsoft that takes you from the fundamentals of Generative AI like building your own, propmt concept, fine tuning to sophisticated concepts like Retrieval-Augmented Generation (RAG), securing generative AI applications, and LLMOps.\nThis resource offers a step-by-step approach, code samples and vedios accessible while preparing you for real-world challenges.\nWhat‚Äôs inside? # What‚Äôs inside? # 21 lessons labeled \u0026ldquo;Learn\u0026rdquo; or \u0026ldquo;Build\u0026rdquo; Python and TypeScript code examples to explain concepts Interactive exercises and the possibility to meet other classmates and get support through an official Discord server Start learning today! # Explore the full course and GitHub repository here: Generative AI for Beginners\nAdditionally, if you\u0026rsquo;re interested in exploring Machine Learning (ML) and MLOps, Microsoft offers a fantastic path to get started. Check out the ML for Beginners GitHub repository for more hands-on resources to deepen your understanding of ML and MLOps.\n","date":"21 January 2025","externalUrl":null,"permalink":"/posts/2025-01-21-a-complete--generative-ai-learning-resource-for-beginners-and-beyond/","section":"Posts","summary":"","title":"A Complete  Generative AI Learning Resource for Beginners and Beyond","type":"posts"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/","section":"Agentify Anchor","summary":"","title":"Agentify Anchor","type":"page"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aiapplications/","section":"Tags","summary":"","title":"AIApplications","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aidevelopment/","section":"Tags","summary":"","title":"AIDevelopment","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aiforbeginners/","section":"Tags","summary":"","title":"AIforBeginners","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aiforbusiness/","section":"Tags","summary":"","title":"AIForBusiness","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aimodels/","section":"Tags","summary":"","title":"AIModels","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/airesources/","section":"Tags","summary":"","title":"AIResources","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/aitraining/","section":"Tags","summary":"","title":"AITraining","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/artificialintelligence/","section":"Tags","summary":"","title":"ArtificialIntelligence","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/codingforai/","section":"Tags","summary":"","title":"CodingForAI","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/deeplearning/","section":"Tags","summary":"","title":"DeepLearning","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/generativeai/","section":"Tags","summary":"","title":"GenerativeAI","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/innovationinai/","section":"Tags","summary":"","title":"InnovationInAI","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/learnai/","section":"Tags","summary":"","title":"LearnAI","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/llmops/","section":"Tags","summary":"","title":"LLMOps","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/machinelearning/","section":"Tags","summary":"","title":"MachineLearning","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"RAG","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/secureai/","section":"Tags","summary":"","title":"SecureAI","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"21 January 2025","externalUrl":null,"permalink":"/tags/techeducation/","section":"Tags","summary":"","title":"TechEducation","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/bot/","section":"Tags","summary":"","title":"Bot","type":"tags"},{"content":"In this blog post, I will guide you through creating a bot-based message extension for Microsoft Teams, without the necessity of the Microsoft 365 Copilot license or waiting until joining the Microsoft 365 Developer Technology Adoption Program (TAP)\nWhile Microsoft 365 Copilot provides enhanced features for enterprise users, you can still build powerful bots using Ollama, a locally run model like Llama3.2, to handle natural language processing.\n1. Setting Up Ollama Locally Using Docker Compose # In this section, I‚Äôll walk you through setting up Ollama locally using Docker Compose. This step-by-step guide will show you how to run Ollama with the Llama3.2 model, providing you with a local environment to process natural language queries for your Microsoft Teams bot.\nMy approach to setting up a local environment is inspired by the n8n self-hosted AI starter kit.\nThe main idea behind was to separate the main Ollama service from the initialization service that pulls models.\ngraph TD A[Ollama_Main_API_Service] --\u003e|Port 11434| B[Ollama_Model_Server_Llama_Models] B --\u003e C[Ollama_Pull_Models_Service] C --\u003e D[Downloads_Models_llama3.2_llama2_nomic_embed_text] A --\u003e|Exposes_API| B C --\u003e|Depends_on| A C --\u003e|Downloads_Models| B style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#ccf,stroke:#333,stroke-width:2px style C fill:#cfc,stroke:#333,stroke-width:2px style D fill:#fcf,stroke:#333,stroke-width:2px The main Ollama (API service) runs the Ollama model server and exposes the API on port 11434.\nThe second instance (init-ollama) is responsible for pulling the models (e.g., llama3.2) when the container starts.\nThe steps to run environment is quite simple, download the Docker Compose YAML file and run:\ndocker compose --profile cpu up Note If you are interested to GPU you can upgrade this or inspiring from the n8n template 1.1 check model pulling # After a while, we can check our service by running the curl command:\ncurl -X GET http://localhost:11434/v1/models If the model(s) are available, we should get a response as shown below:\n1.2 check completions endpoint # After the model loads successfully, we can proceed with running some prompts:\ncurl -X POST http://localhost:11434/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello, Ollama!\u0026#34;}] }\u0026#39; 2. Updating the LlamaModel.ts # After setting up Ollama, we will go through the necessary updates to the original LlamaModel.ts file.\nI‚Äôve overridden the completePrompt method to adjust the payload structure to properly interact with Ollama‚Äôs API endpoints.\nThe most important change is in the payload used to communicate with the service endpoint. I‚Äôve modified it from:\n{ input_data: { input_string: result.output, parameters: template.config.completion } } to use the right format\nconst adjustedPayload = { model: \u0026#34;llama3.2:latest\u0026#34;, // Use the correct model identifier prompt: result.output.map(msg =\u0026gt; msg.content).join(\u0026#39; \u0026#39;), // Flatten messages into a single prompt string max_tokens: template.config.completion.max_tokens || 50, temperature: template.config.completion.temperature || 0.7, //completion_type: \u0026#34;chat\u0026#34;, }; And instead of making the existing request:\nawait this._httpClient.post\u0026lt;{ output: string }\u0026gt;(this.options.endpoint, { input_data: { input_string: result.output, parameters: template.config.completion } }); I use my adjustedPayload\nawait this.llamaModel[\u0026#39;_httpClient\u0026#39;].post(this.llamaModel.options.endpoint, adjustedPayload); And that\u0026rsquo;s it! üòâ\n3. Integrating with Microsoft Teams Samples # To simplify my testing, I used the existing concept sample provided by the Microsoft Teams AI team.\nThis sample typically requires an Azure subscription and use Azure Open AI Studio to create a Llama model. However, in our case, we\u0026rsquo;re doing it for free üòÅ by using our local platform and updating the code to make it work seamlessly.\nTo start using LlamaModelLocal with your MS Teams agent, just download the file, add it to your project\u0026rsquo;s src directory, and import the new LlamaModelLocal instead of the original LlamaModel:\nimport { LlamaModelLocal } from \u0026#34;./oLlamaModel\u0026#34;; Next, replace ts const model = new LlamaModel with ts const model = new LlamaModelLocal Finally, define LLAMA_ENDPOINT in your .env file to point to v1/completions\nLLAMA_ENDPOINT= http://localhost:11434/v1/completions 4. Testing and Debugging with Microsoft Teams Developer Tools # Finally, we will be able to go through testing and debugging our bot using the Teams app test tool.\nWe can start debugging by simply hitting F5 or click start debugging in RUN menu in VsCode.\nNote: Make sure to run npm i or yarn i before starting debugging. And here we go! An instance of the Teams app test tool should open at http://localhost:some_port_number/.\nNow, let\u0026rsquo;s dive into testing our agent!\nSince I\u0026rsquo;ve enabled the logRequests feature and I\u0026rsquo;m running in debug mode, I have access to the secret word. Yes, I know it\u0026rsquo;s a bit like cheating, but I couldn\u0026rsquo;t resist üòá.\nBecause cost matters, this approach allows us to start developing a Microsoft Teams Agent using natural language processing locally with Ollama, without relying on Microsoft 365 Copilot licenses. It‚Äôs a cost-effective, easy-to-setup solution for building and testing Teams agents.\nDemo and Code Sources: # The complete source code used in this demo can be found in this GitHub repository.\nIn the next post, we‚Äôll explore how to create a custom engine agent, deploy it to Microsoft Teams, and use Dev Tunnel SDK to expose publicly our local Ollama endpoint to the internet.\nStay tuned! üëã\n","date":"20 January 2025","externalUrl":null,"permalink":"/posts/2025-01-20-building-a-free-bot-based-message-extension-for-microsoft-teams-without-microsoft-365-copilot-leveraging-ollama-with-llama32/","section":"Posts","summary":"","title":"Building a Free bot-based message extension agent for Microsoft Teams Without Microsoft 365 Copilot: Leveraging Ollama with Llama3.2","type":"posts"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/copilot/","section":"Tags","summary":"","title":"Copilot","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/llama3.2/","section":"Tags","summary":"","title":"Llama3.2","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/m365/","section":"Tags","summary":"","title":"M365","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/message-extension/","section":"Tags","summary":"","title":"Message Extension","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/microsoft-teams/","section":"Tags","summary":"","title":"Microsoft Teams","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/teams-app-test-tool/","section":"Tags","summary":"","title":"Teams App Test Tool","type":"tags"},{"content":"","date":"20 January 2025","externalUrl":null,"permalink":"/tags/teams-toolkit/","section":"Tags","summary":"","title":"Teams Toolkit","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/devcontainer/","section":"Tags","summary":"","title":"DevContainer","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"To conclude this series on using UV for developing Python projects within a portable environment using containers, we will create a sample project and experiment with UV.\nIn our scenario, we will keep it simple. Our requirement is as follows: A store needs an API to return a list of products and filter these products by availability.\nTo get started, we need to set up our project and install the necessary dependencies. We will use FastAPI to create our API and Ruff as a linter to ensure code quality. Follow the steps below to initialize the project, install dependencies, and run the application.\nSteps to Follow: # Create the project. Install dependencies (FastAPI \u0026amp; uvicorn ). Install the linter (Ruff). Run the application. Explore the project structure and files. Create the Project # Run the command to initialize the project:\nuv init Store_api Explore the Project Structure # UV will create a project folder for us. It will generate the .gitignore file to manage which files or folders will be included or excluded from being pushed to our repository, .Python-version which mentions the global version of Python used, a hello world Python script, but the most important file here is pyproject.toml, which defines the project\u0026rsquo;s behavior and definition. pyproject.toml # [project] name = \u0026#34;store-api\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.12\u0026#34; dependencies = [] The pyproject.toml file specifies the required Python version for the project and lists the dependencies. Currently, the dependencies list is empty.\nAdd Dependencies (FastAPI \u0026amp; uvicorn) # As mentioned, we need FastAPI to create our API. To install the packages, navigate to the root project folder and run the following command:\nuv add fastapi uvicorn As you can see, it was so fast.\nTo quickly test the application, simply replace the content of hello.py with the following:\nfrom fastapi import FastAPI from typing import List, Optional from pydantic import BaseModel import uvicorn # Pydantic model for product response class ProductResponse(BaseModel): id: int name: str available: bool # Product class for internal data representation class Product: def __init__(self, id: int, name: str, available: bool): self.id = id self.name = name self.available = available # Sample products products = [ Product(1, \u0026#34;Jolly Jester Clown Wig\u0026#34;, True), Product(2, \u0026#34;Bozo the Clown Nose\u0026#34;, False), Product(3, \u0026#34;Circus Performer Clown Shoes\u0026#34;, True), Product(4, \u0026#34;Red Balloon Animal Kit\u0026#34;, True), Product(5, \u0026#34;Funny Clown Face Paint Set\u0026#34;, True), Product(6, \u0026#34;Mini Clown Horn\u0026#34;, False), Product(7, \u0026#34;Rainbow Clown Costume\u0026#34;, True), Product(8, \u0026#34;Clown Magician Hat\u0026#34;, True), Product(9, \u0026#34;Giggles the Clown Plush Doll\u0026#34;, True), Product(10, \u0026#34;Clown Comedy Seltzer Bottle\u0026#34;, False) ] # FastAPI app initialization app = FastAPI() @app.get(\u0026#34;/products\u0026#34;, response_model=List[ProductResponse]) def get_products(available: Optional[bool] = None): # If \u0026#39;available\u0026#39; query param is provided, filter products by availability filtered_products = products if available is None else [product for product in products if product.available == available] # Return products as list of Pydantic model instances return [ProductResponse(**product.__dict__) for product in filtered_products] @app.get(\u0026#34;/\u0026#34;) def main(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from store-api!\u0026#34;} if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) #main() # to run using command line `uv run uvicorn hello:app --port 8000` Installing and Using Tool (Ruff) # Every project needs some tooling like Ruff for linting. We can manage that by adding the tool as a dev dependency using uv add --dev ruff or just use uvx. Run the Application # To run the application, just use:\nuv run hello.py Click on the \u0026ldquo;Open in Browser\u0026rdquo; button from the popup. We don\u0026rsquo;t need to make any additional configuration for port forwarding. UV also creates and manages the .venv folder for us. upgrade / downgrade Python in a project # To upgrade or downgrade the Python version, simply update the version in the pyproject.toml file and run\nuv sync If you remember, we configured our initial environment to use Python 3.12, but we can use any version in our project. We don\u0026rsquo;t need to manage installation or set up multiple Python versions ‚Äî UV does the job for us.\nIn this post, we have successfully set up a Python project using UV, ensuring that we can go live with our container and start our productive approach with Dev Containers.\nYou can find the complete source code for this demo on GitHub. If you‚Äôre a fan of Dockerfiles and prefer to manage your development environment manually, refer to this project, which uses the official UV Dockerfile definition.\nCheers! üçª\n","date":"12 January 2025","externalUrl":null,"permalink":"/posts/2025-01-12-getting-started-with-uv-in-docker-step-3-explained/","section":"Posts","summary":"","title":"Getting Started with UV in Docker","type":"posts"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/podman/","section":"Tags","summary":"","title":"Podman","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/vscode/","section":"Tags","summary":"","title":"VSCode","type":"tags"},{"content":"","date":"12 January 2025","externalUrl":null,"permalink":"/tags/wsl/","section":"Tags","summary":"","title":"WSL","type":"tags"},{"content":"","date":"8 January 2025","externalUrl":null,"permalink":"/posts/2025-01-08-run-mysql-in-a-docker-container-a-beginners-guide/","section":"Posts","summary":"","title":"Run MySQL in a Docker Container: A Beginner's Guide","type":"posts"},{"content":"In our series exploring how to set up a streamlined and organized way to start your Python development journey using the DevContainer VSCode extension and Docker (or WSL/Podman), we\u0026rsquo;ve already seen how simple it is to define our development environment using just the DevContainer extension. Now, we\u0026rsquo;re ready to move on to the next part of the series: Optimize Python Development with VSCode and Docker.\nRunning Our Environment for the First Time # In this post, we will run our development environment for the first time and start working with Python using an amazing package and project manager, uv.\nAfter generating the definition for our environment, we now have a devcontainer.json file structure. Let‚Äôs dig in to explore what we have in this setup:\nBreaking Down the devcontainer.json Structure # Image Section:\nThis section specifies the container image reference, which is hosted on the Microsoft Container Registry.\nNote: You can also use a local image if you prefer. use DockerFile or Docker compose file.\nFeatures Section:\nThink of this section as a way to enhance the environment with additional tools and packages. In our case, we‚Äôre using uv (with shellautocompletion option enbaled).\ncustomization Section: The customization section is used to fine-tune the container development environment. It allows us to install additional extensions that will only be loaded inside the container, apply specific settings related to VSCode preferences, and execute additional commands.\nFor more information on available features, visit the Dev Containers Features Documentation. # If you\u0026rsquo;re working with a rootless container, you can find relevant information. here.\nLaunching Our Environment for the First Time üöÄ # Now it\u0026rsquo;s time to launch our environment for the first time! To do this, simply click on the Dev Container icon and choose Reopen in Container.\nIn the background:\nAn image will be added to your Docker images list. A container should be running. A volume will be created. VSCode will reload, but this time, it will run under the container. Time to Verify # The Remote Indicator in the status bar will display additional information, such as the name of the container we\u0026rsquo;re currently in and the distribution of the image being used.\nVerifying Python Version and Testing uv # Let‚Äôs perform a couple more checks by verifying the Python version being used and testing if uv is running correctly.\nEverything seems good so far! The next step is to create our first project and run it. That‚Äôs exactly what we‚Äôll cover in the next post.\nStay tuned!\n","date":"8 January 2025","externalUrl":null,"permalink":"/posts/2025-01-08-streamline-python-development-in-devcontainer-with-uv/","section":"Posts","summary":"","title":"Streamline Python Development in DevContainer with UV","type":"posts"},{"content":"In today\u0026rsquo;s world, Python is used for many purposes in large projects, especially in AI, LLM, and RAG. However, maintaining a healthy environment, managing compatibility, and handling packaging can be challenging tasks.\nFor this reason, using containers not only for deploying Python apps but also as a development environment could be a suitable approach.\nIn this post, we will show how simple it is to start developing Python projects without needing to install Python, manage multiple versions, create virtual environments, set environment variables, and handle other prerequisites.\nWe can achieve this directly from VSCode using just one extension.\nPresenting the Extension # We‚Äôve probably all seen the .devcontainer folder and the devcontainer.json file in other projects. But what do they do?\nThe .devcontainer folder and its JSON file are used to define a containerized development environment. This setup ensures that all developers on a project have a consistent development environment, regardless of their local machine configurations. The .devcontainer configuration can specify the base image, extensions, and settings needed for the project.\nWe‚Äôre going to make this easy by generating the configuration files automatically‚Äîno need to craft them manually from scratch! üòâ\nSo let\u0026rsquo;s get started\u0026hellip;\nPrerequisites # Before we dive in, make sure we have Docker and VSCode installed.\nStep 1: Installing the Extension # First, let‚Äôs install the necessary extension in VSCode. We can do this by visiting the Dev Container extension page or by searching for it directly in the extension manager in Visual Studio Code. Use the Ctrl+Shift+X shortcut to open the extension manager.\nStep 2: Generate the DevContainer Folder # After installing the extension, we should find an icon at the bottom left side of our IDE. Click on it and follow the wizard steps to generate the .devcontainer folder and configuration files.\nSelect Add Dev Container Configuration Files.\nThen select Add Configuration to Workspace. This will add the .devcontainer folder and related configuration to our current project directory.\nWe can choose from a variety of preset configurations. In our case, we will select Python 3.\nWe‚Äôll be prompted to choose the Python version for your container. Pick any version you‚Äôre comfortable with (we‚Äôll go with Python 3.12 bullseye).\nIn the following step, we can choose additional features. We‚Äôll select uv, a great package and project manager that we‚Äôll use further in this post.\nWe can also select additional configurations for the features, so let‚Äôs enable the shell autocompletion option.\nWe can skip the next step if we don\u0026rsquo;t want to use the Dependabot functionality. Dependabot helps us keep our dependencies up to date by automatically checking for updates and creating pull requests. After running the wizard, we‚Äôll end up with the following root project structure.\nüéâ Once the .devcontainer folder is set up, there‚Äôs no need to build an image manually‚Äîthe extension will handle that for us! We‚Äôre now ready to connect to the containerized environment and start using it as our development environment. We‚Äôll explore this process in Part 2 of this post, so stay tuned!\nHappy coding, and see you in the next part!\n","date":"5 January 2025","externalUrl":null,"permalink":"/posts/2025-01-05-optimize-python-development-with-docker/","section":"Posts","summary":"","title":"Optimize Python Development with VSCode and Docker","type":"posts"},{"content":"","date":"5 January 2025","externalUrl":null,"permalink":"/tags/aboutme/","section":"Tags","summary":"","title":"Aboutme","type":"tags"},{"content":"","date":"5 January 2025","externalUrl":null,"permalink":"/tags/agentifyanchor/","section":"Tags","summary":"","title":"Agentifyanchor","type":"tags"},{"content":"Hello! üëã\nI\u0026rsquo;m a Microsoft Specialist üíª with a deep passion for AI ü§ñ. Over the years, I\u0026rsquo;ve become fascinated by optimizing development environments, especially in the world of Python üêç, Docker üêã, and VSCode üñ•Ô∏è. I‚Äôm always looking for ways to simplify complex tasks and make development smoother, which is why I love using containers and powerful tools to create consistent and portable setups.\nMy journey is all about exploring new technologies, automating workflows, and applying AI in innovative ways. I‚Äôm excited to share my insights and experiences with you, and I hope to learn and grow together in this tech journey! üåü\nDon\u0026rsquo;t forget to follow me on Twitter. üê¶ ","date":"5 January 2025","externalUrl":null,"permalink":"/posts/discover-journey/","section":"Posts","summary":"","title":"Discover My Journey: About Me","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]